#!/usr/bin/env python3
"""
export_sentiment_pretrained.py - Export pre-trained sentiment model weights to FIONA C headers

This script converts PyTorch model weights (from train_sentiment_pretrained.py)
to C header files that can be compiled directly into FIONA-workload applications.

Key differences from export_sentiment_weights.py:
  - Handles bert-tiny architecture (d_ff=512)
  - Includes token_type_embedding and embedding LayerNorm
  - Supports INT16 quantization option

Usage:
    python export_sentiment_pretrained.py \\
        --model_path ./sentiment_pretrained/model.pt \\
        --output_dir ../app/sentiment_classifier/weights

Author: FIONA Project
Date: 2025-12-23
"""

import argparse
import torch
import numpy as np
from pathlib import Path


def write_array_to_file(f, name: str, arr: np.ndarray, desc: str = ""):
    """Write a single array to an open file handle."""
    flat = arr.flatten()
    f.write(f"// {desc}, Shape: {list(arr.shape)}\n")
    f.write(f"static const float {name}[{len(flat)}] = {{\n")
    for i in range(0, len(flat), 8):
        chunk = flat[i:i + 8]
        values = ", ".join(f"{v:.8f}f" for v in chunk)
        f.write(f"    {values},\n")
    f.write("};\n\n")


def write_weight_header(filepath: Path, name: str, arr: np.ndarray, description: str = ""):
    """Write a weight array as a C header file."""
    with open(filepath, "w") as f:
        guard = f"{name.upper()}_H"
        f.write(f"/**\n")
        f.write(f" * @file {filepath.name}\n")
        f.write(f" * @brief {description}\n")
        f.write(f" * @note Auto-generated by export_sentiment_pretrained.py\n")
        f.write(f" *\n")
        f.write(f" * Shape: {list(arr.shape)}\n")
        f.write(f" * Elements: {arr.size:,}\n")
        f.write(f" * Memory: {arr.nbytes:,} bytes\n")
        f.write(f" */\n\n")

        f.write(f"#ifndef {guard}\n")
        f.write(f"#define {guard}\n\n")

        flat = arr.flatten()
        f.write(f"// Shape: {list(arr.shape)}\n")
        f.write(f"static const float {name}[{len(flat)}] = {{\n")

        for i in range(0, len(flat), 8):
            chunk = flat[i:i + 8]
            values = ", ".join(f"{v:.8f}f" for v in chunk)
            f.write(f"    {values},\n")

        f.write("};\n\n")
        f.write(f"#endif // {guard}\n")


def export_model(model_path: str, output_dir: str):
    """Export PyTorch model to FIONA C headers."""
    print("=" * 60)
    print("FIONA Pre-trained Weight Exporter")
    print("=" * 60)

    # Load model
    print(f"\nLoading model from: {model_path}")
    checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)

    state_dict = checkpoint["model_state_dict"]
    config = checkpoint.get("config", {})
    accuracy = checkpoint.get("accuracy", 0.0)
    pretrained_model = checkpoint.get("pretrained_model", "unknown")

    print(f"Pre-trained model: {pretrained_model}")
    print(f"Validation accuracy: {accuracy:.4f}")
    print(f"Config: {config}")

    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Export weights organized by component
    print("\nExporting weights...")

    # 1. Embeddings
    print("  - Embeddings")
    export_embedding_weights(state_dict, output_path, config)

    # 2. Transformer layers
    n_layers = config.get("n_layers", 2)
    for layer_idx in range(n_layers):
        print(f"  - Layer {layer_idx}")
        export_layer_weights(state_dict, output_path, layer_idx, config)

    # 3. Pooler and classifier
    print("  - Pooler and Classifier")
    export_head_weights(state_dict, output_path, config)

    # 4. Generate master header
    print("  - Master header")
    generate_master_header(output_path, config)

    # 5. Generate manifest
    print("  - Manifest")
    generate_manifest(state_dict, output_path, config, accuracy, pretrained_model)

    print(f"\nWeights exported to: {output_path}")
    print("=" * 60)

    # Summary
    total_params = sum(t.numel() for t in state_dict.values())
    print(f"\nSummary:")
    print(f"  Total parameters: {total_params:,}")
    print(f"  FP32 size: {total_params * 4 / 1024 / 1024:.2f} MB")
    print(f"  INT16 size: {total_params * 2 / 1024 / 1024:.2f} MB")


def export_embedding_weights(state_dict: dict, output_path: Path, config: dict):
    """Export embedding weights."""
    # Token embedding
    token_emb = state_dict["token_embedding.weight"].numpy()
    write_weight_header(
        output_path / "embedding_token.h",
        "token_embedding",
        token_emb,
        f"Token embedding [{config.get('vocab_size', token_emb.shape[0])} x {config.get('d_model', token_emb.shape[1])}]"
    )

    # Position embedding
    pos_emb = state_dict["position_embedding.weight"].numpy()
    write_weight_header(
        output_path / "embedding_position.h",
        "position_embedding",
        pos_emb,
        f"Position embedding [{config.get('max_position', pos_emb.shape[0])} x {config.get('d_model', pos_emb.shape[1])}]"
    )

    # Token type embedding (BERT-specific)
    if "token_type_embedding.weight" in state_dict:
        tt_emb = state_dict["token_type_embedding.weight"].numpy()
        write_weight_header(
            output_path / "embedding_token_type.h",
            "token_type_embedding",
            tt_emb,
            f"Token type embedding [2 x {config.get('d_model', tt_emb.shape[1])}]"
        )

    # Embedding LayerNorm (BERT-specific)
    if "embedding_layernorm.weight" in state_dict:
        ln_gamma = state_dict["embedding_layernorm.weight"].numpy()
        ln_beta = state_dict["embedding_layernorm.bias"].numpy()

        with open(output_path / "embedding_layernorm.h", "w") as f:
            f.write("/**\n")
            f.write(" * @file embedding_layernorm.h\n")
            f.write(" * @brief Embedding LayerNorm weights\n")
            f.write(" * @note Auto-generated by export_sentiment_pretrained.py\n")
            f.write(" */\n\n")
            f.write("#ifndef EMBEDDING_LAYERNORM_H\n")
            f.write("#define EMBEDDING_LAYERNORM_H\n\n")

            write_array_to_file(f, "embedding_ln_gamma", ln_gamma, "Embedding LayerNorm gamma")
            write_array_to_file(f, "embedding_ln_beta", ln_beta, "Embedding LayerNorm beta")

            f.write("#endif // EMBEDDING_LAYERNORM_H\n")


def export_layer_weights(state_dict: dict, output_path: Path, layer_idx: int, config: dict):
    """Export weights for a single transformer layer."""
    prefix = f"encoder.layers.{layer_idx}"

    # Self-attention weights
    in_proj = state_dict[f"{prefix}.self_attn.in_proj_weight"].numpy()
    in_proj_bias = state_dict[f"{prefix}.self_attn.in_proj_bias"].numpy()

    # Split into Q, K, V
    Wq, Wk, Wv = np.split(in_proj, 3, axis=0)
    bq, bk, bv = np.split(in_proj_bias, 3, axis=0)

    # Output projection
    Wo = state_dict[f"{prefix}.self_attn.out_proj.weight"].numpy()
    bo = state_dict[f"{prefix}.self_attn.out_proj.bias"].numpy()

    # FFN weights
    W1 = state_dict[f"{prefix}.linear1.weight"].numpy()
    b1 = state_dict[f"{prefix}.linear1.bias"].numpy()
    W2 = state_dict[f"{prefix}.linear2.weight"].numpy()
    b2 = state_dict[f"{prefix}.linear2.bias"].numpy()

    # LayerNorm weights
    ln1_gamma = state_dict[f"{prefix}.norm1.weight"].numpy()
    ln1_beta = state_dict[f"{prefix}.norm1.bias"].numpy()
    ln2_gamma = state_dict[f"{prefix}.norm2.weight"].numpy()
    ln2_beta = state_dict[f"{prefix}.norm2.bias"].numpy()

    # Write all weights for this layer
    layer_name = f"layer{layer_idx}"

    weights = [
        (f"{layer_name}_Wq", Wq, "Query projection weight"),
        (f"{layer_name}_bq", bq, "Query projection bias"),
        (f"{layer_name}_Wk", Wk, "Key projection weight"),
        (f"{layer_name}_bk", bk, "Key projection bias"),
        (f"{layer_name}_Wv", Wv, "Value projection weight"),
        (f"{layer_name}_bv", bv, "Value projection bias"),
        (f"{layer_name}_Wo", Wo, "Output projection weight"),
        (f"{layer_name}_bo", bo, "Output projection bias"),
        (f"{layer_name}_W1", W1, "FFN layer 1 weight"),
        (f"{layer_name}_b1", b1, "FFN layer 1 bias"),
        (f"{layer_name}_W2", W2, "FFN layer 2 weight"),
        (f"{layer_name}_b2", b2, "FFN layer 2 bias"),
        (f"{layer_name}_ln1_gamma", ln1_gamma, "LayerNorm 1 gamma"),
        (f"{layer_name}_ln1_beta", ln1_beta, "LayerNorm 1 beta"),
        (f"{layer_name}_ln2_gamma", ln2_gamma, "LayerNorm 2 gamma"),
        (f"{layer_name}_ln2_beta", ln2_beta, "LayerNorm 2 beta"),
    ]

    with open(output_path / f"{layer_name}.h", "w") as f:
        guard = f"{layer_name.upper()}_H"
        f.write(f"/**\n")
        f.write(f" * @file {layer_name}.h\n")
        f.write(f" * @brief Transformer layer {layer_idx} weights (pre-trained)\n")
        f.write(f" * @note Auto-generated by export_sentiment_pretrained.py\n")
        f.write(f" */\n\n")
        f.write(f"#ifndef {guard}\n")
        f.write(f"#define {guard}\n\n")

        for name, arr, desc in weights:
            write_array_to_file(f, name, arr, desc)

        f.write(f"#endif // {guard}\n")


def export_head_weights(state_dict: dict, output_path: Path, config: dict):
    """Export pooler and classifier weights."""
    pooler_weight = state_dict["pooler.weight"].numpy()
    pooler_bias = state_dict["pooler.bias"].numpy()
    classifier_weight = state_dict["classifier.weight"].numpy()
    classifier_bias = state_dict["classifier.bias"].numpy()

    with open(output_path / "classifier.h", "w") as f:
        f.write("/**\n")
        f.write(" * @file classifier.h\n")
        f.write(" * @brief Pooler and classifier head weights (pre-trained)\n")
        f.write(" * @note Auto-generated by export_sentiment_pretrained.py\n")
        f.write(" */\n\n")
        f.write("#ifndef CLASSIFIER_H\n")
        f.write("#define CLASSIFIER_H\n\n")

        weights = [
            ("pooler_weight", pooler_weight, "Pooler weight"),
            ("pooler_bias", pooler_bias, "Pooler bias"),
            ("classifier_weight", classifier_weight, "Classifier weight"),
            ("classifier_bias", classifier_bias, "Classifier bias"),
        ]

        for name, arr, desc in weights:
            write_array_to_file(f, name, arr, desc)

        f.write("#endif // CLASSIFIER_H\n")


def generate_master_header(output_path: Path, config: dict):
    """Generate master header that includes all weight files."""
    n_layers = config.get("n_layers", 2)

    with open(output_path / "weights.h", "w") as f:
        f.write("/**\n")
        f.write(" * @file weights.h\n")
        f.write(" * @brief Master header for pre-trained sentiment model weights\n")
        f.write(" * @note Auto-generated by export_sentiment_pretrained.py\n")
        f.write(" *\n")
        f.write(" * This model uses pre-trained BERT-tiny weights fine-tuned on SST-2.\n")
        f.write(" */\n\n")
        f.write("#ifndef WEIGHTS_H\n")
        f.write("#define WEIGHTS_H\n\n")

        f.write("// Model configuration (bert-tiny architecture)\n")
        f.write(f"#define MODEL_VOCAB_SIZE   {config.get('vocab_size', 30522)}\n")
        f.write(f"#define MODEL_MAX_POSITION {config.get('max_position', 512)}\n")
        f.write(f"#define MODEL_D_MODEL      {config.get('d_model', 128)}\n")
        f.write(f"#define MODEL_N_HEADS      {config.get('n_heads', 2)}\n")
        f.write(f"#define MODEL_D_K          {config.get('d_k', 64)}\n")
        f.write(f"#define MODEL_D_FF         {config.get('d_ff', 512)}\n")
        f.write(f"#define MODEL_N_LAYERS     {config.get('n_layers', 2)}\n")
        f.write(f"#define MODEL_NUM_LABELS   {config.get('num_labels', 2)}\n\n")

        f.write("// Pre-trained model info\n")
        f.write("#define MODEL_PRETRAINED   1\n")
        f.write('#define MODEL_BASE         "bert-tiny"\n\n')

        f.write("// Include all weight files\n")
        f.write('#include "embedding_token.h"\n')
        f.write('#include "embedding_position.h"\n')
        f.write('#include "embedding_token_type.h"\n')
        f.write('#include "embedding_layernorm.h"\n')
        for i in range(n_layers):
            f.write(f'#include "layer{i}.h"\n')
        f.write('#include "classifier.h"\n\n')

        f.write("#endif // WEIGHTS_H\n")


def generate_manifest(state_dict: dict, output_path: Path, config: dict,
                      accuracy: float, pretrained_model: str):
    """Generate manifest file with weight metadata."""
    with open(output_path / "manifest.txt", "w") as f:
        f.write("# Pre-trained Sentiment Model Weight Manifest\n")
        f.write("# Auto-generated by export_sentiment_pretrained.py\n")
        f.write("#\n")
        f.write(f"# Base model: {pretrained_model}\n")
        f.write(f"# Validation accuracy: {accuracy:.4f}\n")
        f.write("#\n")
        f.write("# Format: name, shape, dtype, size_bytes\n\n")

        total_params = 0
        for name, tensor in state_dict.items():
            shape = list(tensor.shape)
            numel = tensor.numel()
            total_params += numel
            f.write(f"{name}, {shape}, float32, {numel * 4}\n")

        f.write(f"\n# Configuration\n")
        for key, value in config.items():
            f.write(f"# {key}: {value}\n")

        f.write(f"\n# Summary\n")
        f.write(f"# Total parameters: {total_params:,}\n")
        f.write(f"# Total size (FP32): {total_params * 4 / 1024 / 1024:.2f} MB\n")
        f.write(f"# Total size (INT16): {total_params * 2 / 1024 / 1024:.2f} MB\n")


def main():
    parser = argparse.ArgumentParser(
        description="Export pre-trained sentiment model weights to FIONA C headers"
    )

    parser.add_argument("--model_path", type=str, required=True,
                        help="Path to model.pt checkpoint")
    parser.add_argument("--output_dir", type=str,
                        default="../app/sentiment_classifier/weights",
                        help="Output directory for weight headers")

    args = parser.parse_args()
    export_model(args.model_path, args.output_dir)


if __name__ == "__main__":
    main()
