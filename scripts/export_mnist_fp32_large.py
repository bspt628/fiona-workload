#!/usr/bin/env python3
"""
Export MNIST MLP weights and test data in FP32 format for paper-level validation.

Generates:
- FP32 weights (no quantization)
- 1000+ test samples
- Per-class statistics for academic validation

Usage:
    python export_mnist_fp32_large.py --num-samples 1000 --output ../app/mlp_mnist_photonic/
"""

import argparse
import numpy as np
import os

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader
    import torchvision
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Error: PyTorch is required")
    exit(1)


class SimpleMLP(nn.Module):
    """Simple MLP for MNIST."""
    def __init__(self, input_size=784, hidden_sizes=[256, 128, 64], output_size=10):
        super().__init__()
        layers = []
        prev_size = input_size
        for h in hidden_sizes:
            layers.append(nn.Linear(prev_size, h))
            layers.append(nn.ReLU())
            prev_size = h
        layers.append(nn.Linear(prev_size, output_size))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x.view(x.size(0), -1))


def train_mnist_model(hidden_sizes, epochs=15, device='cpu'):
    """Train MLP on MNIST."""
    print(f"Training MLP on MNIST (hidden={hidden_sizes}, epochs={epochs})...")

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=transform)
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    model = SimpleMLP(784, hidden_sizes, 10).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # Train
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"  Epoch {epoch+1}/{epochs}: Loss = {total_loss/len(train_loader):.4f}")

    # Evaluate
    model.eval()
    correct = 0
    total = 0
    class_correct = [0] * 10
    class_total = [0] * 10

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)

            for i in range(target.size(0)):
                label = target[i].item()
                class_total[label] += 1
                if pred[i] == target[i]:
                    class_correct[label] += 1

    accuracy = 100.0 * correct / total
    print(f"\nTest Accuracy: {accuracy:.2f}% ({correct}/{total})")
    print("\nPer-class accuracy:")
    for i in range(10):
        class_acc = 100.0 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        print(f"  Class {i}: {class_acc:.2f}% ({class_correct[i]}/{class_total[i]})")

    return model, test_dataset, accuracy


def extract_weights_fp32(model):
    """Extract weights as FP32 numpy arrays."""
    weights = []
    biases = []

    for module in model.modules():
        if isinstance(module, nn.Linear):
            w = module.weight.detach().cpu().numpy().astype(np.float32)
            b = module.bias.detach().cpu().numpy().astype(np.float32) if module.bias is not None else None
            weights.append(w)
            biases.append(b)

    return weights, biases


def generate_fp32_header(weights, biases, test_x, test_y, output_dir, num_samples, pytorch_accuracy):
    """Generate FP32 C header files."""

    os.makedirs(output_dir, exist_ok=True)

    # Architecture info
    arch = [weights[0].shape[1]] + [w.shape[0] for w in weights]
    arch_str = ' -> '.join(map(str, arch))

    # === Generate weights header ===
    weights_file = os.path.join(output_dir, 'mnist_fp32_weights.h')
    with open(weights_file, 'w') as f:
        f.write(f"""/**
 * @file mnist_fp32_weights.h
 * @brief FP32 MNIST MLP weights for photonic simulation
 *
 * Architecture: {arch_str}
 * PyTorch Accuracy: {pytorch_accuracy:.2f}%
 * Test Samples: {num_samples}
 *
 * Generated by export_mnist_fp32_large.py
 */

#ifndef MNIST_FP32_WEIGHTS_H
#define MNIST_FP32_WEIGHTS_H

// Architecture parameters
#define MNIST_INPUT_SIZE {weights[0].shape[1]}
""")
        for i, w in enumerate(weights):
            f.write(f"#define MNIST_HIDDEN{i+1}_SIZE {w.shape[0]}\n")
        f.write(f"#define MNIST_OUTPUT_SIZE {weights[-1].shape[0]}\n")
        f.write(f"#define MNIST_NUM_LAYERS {len(weights)}\n\n")

        # Total parameters
        total_params = sum(w.size + (b.size if b is not None else 0) for w, b in zip(weights, biases))
        f.write(f"// Total parameters: {total_params}\n")
        f.write(f"#define MNIST_TOTAL_PARAMS {total_params}\n\n")

        # Write each layer's weights
        for i, (w, b) in enumerate(zip(weights, biases)):
            out_size, in_size = w.shape
            f.write(f"// Layer {i+1}: {in_size} -> {out_size}\n")
            f.write(f"static const float mnist_w{i+1}[{out_size}][{in_size}] = {{\n")
            for row in w:
                f.write("    {" + ", ".join(f"{v:.8f}f" for v in row) + "},\n")
            f.write("};\n\n")

            if b is not None:
                f.write(f"static const float mnist_b{i+1}[{out_size}] = {{\n    ")
                f.write(", ".join(f"{v:.8f}f" for v in b))
                f.write("\n};\n\n")

        f.write("#endif /* MNIST_FP32_WEIGHTS_H */\n")

    print(f"Generated: {weights_file}")

    # === Generate test data header ===
    testdata_file = os.path.join(output_dir, 'mnist_fp32_testdata.h')
    with open(testdata_file, 'w') as f:
        f.write(f"""/**
 * @file mnist_fp32_testdata.h
 * @brief MNIST test data ({num_samples} samples) for paper-level validation
 *
 * Test samples are normalized: (pixel - 0.1307) / 0.3081
 * Labels: 0-9 (digit classes)
 *
 * Generated by export_mnist_fp32_large.py
 */

#ifndef MNIST_FP32_TESTDATA_H
#define MNIST_FP32_TESTDATA_H

#define MNIST_NUM_TEST_SAMPLES {num_samples}

// Test input data: {num_samples} samples x 784 features
static const float mnist_test_x[{num_samples}][{test_x.shape[1]}] = {{
""")
        for i in range(num_samples):
            f.write("    {" + ", ".join(f"{v:.6f}f" for v in test_x[i]) + "},\n")
        f.write("};\n\n")

        f.write(f"// Test labels\n")
        f.write(f"static const int mnist_test_y[{num_samples}] = {{")
        f.write(", ".join(map(str, test_y[:num_samples])))
        f.write("};\n\n")

        # Per-class sample counts for validation
        class_counts = [0] * 10
        for label in test_y[:num_samples]:
            class_counts[label] += 1

        f.write("// Per-class sample counts\n")
        f.write(f"static const int mnist_class_counts[10] = {{{', '.join(map(str, class_counts))}}};\n\n")

        f.write("#endif /* MNIST_FP32_TESTDATA_H */\n")

    print(f"Generated: {testdata_file}")

    # Print per-class distribution
    print(f"\nTest data distribution ({num_samples} samples):")
    for i in range(10):
        print(f"  Class {i}: {class_counts[i]} samples ({100*class_counts[i]/num_samples:.1f}%)")

    return weights_file, testdata_file


def main():
    parser = argparse.ArgumentParser(description='Export MNIST FP32 weights for photonic simulation')
    parser.add_argument('--num-samples', type=int, default=1000,
                        help='Number of test samples (default: 1000)')
    parser.add_argument('--hidden', type=str, default='256,128,64',
                        help='Hidden layer sizes (default: 256,128,64)')
    parser.add_argument('--epochs', type=int, default=15,
                        help='Training epochs (default: 15)')
    parser.add_argument('--output', type=str, default='../app/mlp_mnist_photonic/',
                        help='Output directory')

    args = parser.parse_args()

    hidden_sizes = [int(x) for x in args.hidden.split(',')]

    # Train model
    model, test_dataset, accuracy = train_mnist_model(hidden_sizes, args.epochs)

    # Extract FP32 weights
    weights, biases = extract_weights_fp32(model)

    # Extract test samples
    print(f"\nExtracting {args.num_samples} test samples...")
    test_x = []
    test_y = []

    for i, (data, label) in enumerate(test_dataset):
        if i >= args.num_samples:
            break
        test_x.append(data.numpy().flatten())
        test_y.append(label)

    test_x = np.array(test_x, dtype=np.float32)
    test_y = np.array(test_y, dtype=np.int32)

    # Generate headers
    generate_fp32_header(weights, biases, test_x, test_y,
                         args.output, args.num_samples, accuracy)

    print(f"\n=== Summary ===")
    print(f"Architecture: {' -> '.join(map(str, [784] + hidden_sizes + [10]))}")
    print(f"PyTorch Accuracy: {accuracy:.2f}%")
    print(f"Test samples: {args.num_samples}")
    print(f"Output directory: {args.output}")

    return 0


if __name__ == '__main__':
    exit(main())
