/**
 * @file main.cc
 * @brief Sentiment Classification Benchmark with INT16 Photonic Acceleration
 *
 * Uses FIONA INT16 MVM instructions for photonic acceleration.
 * Implements layer-specific quantization scales for optimal precision.
 *
 * Quantization Strategy:
 * - Embedding: scale=128 (standard range)
 * - QKV projections: scale=64 (smaller to prevent overflow in dot product)
 * - Attention scores: scale=32767 (softmax output [0,1] to full INT16 range)
 * - FFN: scale=128 (standard range)
 * - Output: scale=128 (standard range)
 *
 * @author FIONA Project
 * @date 2025-12-23
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

#include "fiona.h"
#include "nn/transformer_int16.h"

// Include exported weights (FP32)
#include "weights/weights.h"

// Include test data
#include "testdata/sst2_testdata.h"

// ============================================================
// Model Configuration
// ============================================================

// Use configuration from weights.h (generated by export script)
#define D_MODEL        MODEL_D_MODEL
#define N_HEADS        MODEL_N_HEADS
#define D_K            MODEL_D_K
#define D_FF           MODEL_D_FF
#define N_LAYERS       MODEL_N_LAYERS
#define MAX_SEQ_LEN    64
#define NUM_LABELS     MODEL_NUM_LABELS

// ============================================================
// Layer-specific Quantization Configuration
// ============================================================

// Use different name to avoid conflict with QuantConfig in transformer_int16.h
struct LayerScales {
    float embed_scale;      // Embedding layer scale
    float qkv_scale;        // Q, K, V projection scale (smaller for dot product)
    float attn_scale;       // Attention output scale
    float ffn_scale;        // FFN hidden layer scale
    float output_scale;     // General output scale
    float softmax_scale;    // Softmax output scale (probability [0,1])
};

// Default configuration with layer-appropriate scales
static const LayerScales SCALES = {
    128.0f,   // embed_scale
    64.0f,    // qkv_scale - Smaller because Q*K dot product squares the scale
    128.0f,   // attn_scale
    128.0f,   // ffn_scale
    128.0f,   // output_scale
    32767.0f  // softmax_scale - [0,1] mapped to full INT16 range
};

// ============================================================
// 64-byte aligned Static Buffers (INT16)
// ============================================================

static int16_t embeddings_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t hidden_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t hidden2_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t temp_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t normed_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));

static int16_t Q_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t K_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t V_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t attn_scores_q[N_HEADS * MAX_SEQ_LEN * MAX_SEQ_LEN] __attribute__((aligned(64)));
static int16_t attn_out_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));
static int16_t attn_proj_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));

static int16_t ffn_hidden_q[MAX_SEQ_LEN * D_FF] __attribute__((aligned(64)));
static int16_t ffn_out_q[MAX_SEQ_LEN * D_MODEL] __attribute__((aligned(64)));

// Attention mask (1 = valid, 0 = padding)
static int attention_mask[MAX_SEQ_LEN];

// Quantized weight buffers for layers
static int16_t layer0_Wq_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer0_Wk_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer0_Wv_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer0_Wo_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer0_W1_q[D_MODEL * D_FF] __attribute__((aligned(64)));
static int16_t layer0_W2_q[D_FF * D_MODEL] __attribute__((aligned(64)));

static int16_t layer1_Wq_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer1_Wk_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer1_Wv_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer1_Wo_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t layer1_W1_q[D_MODEL * D_FF] __attribute__((aligned(64)));
static int16_t layer1_W2_q[D_FF * D_MODEL] __attribute__((aligned(64)));

static int16_t pooler_w_q[D_MODEL * D_MODEL] __attribute__((aligned(64)));
static int16_t classifier_w_q[NUM_LABELS * D_MODEL] __attribute__((aligned(64)));

// ============================================================
// Quantization Helpers with Scale Parameter
// ============================================================

static inline int16_t quant_s(float val, float scale) {
    int32_t q = (int32_t)roundf(val * scale);
    if (q > 32767) q = 32767;
    if (q < -32768) q = -32768;
    return (int16_t)q;
}

static inline float dequant_s(int16_t val, float scale) {
    return (float)val / scale;
}

// Convenience wrappers using default output scale
static inline int16_t quant(float val) {
    return quant_s(val, SCALES.output_scale);
}

static inline float dequant(int16_t val) {
    return dequant_s(val, SCALES.output_scale);
}

void quantize_weights_s(int16_t* out, const float* in, size_t len, float scale) {
    for (size_t i = 0; i < len; i++) {
        out[i] = quant_s(in[i], scale);
    }
}

// ============================================================
// Photonic Linear Layer with Scale-Aware Processing
// ============================================================

/**
 * Photonic linear layer with proper scale handling
 *
 * MVM computes: sum(input[j] * weight[j]) for each output
 * When input is scaled by in_scale and weight by w_scale:
 *   MVM_result = in_scale * w_scale * sum(x[j] * w[j])
 *
 * We need to rescale to out_scale:
 *   output = (MVM_result / (in_scale * w_scale) + bias) * out_scale
 *          = MVM_result / (in_scale * w_scale) * out_scale + bias * out_scale
 */
void photonic_linear_scaled(int16_t* output, const int16_t* input,
                            const int16_t* weight, const float* bias,
                            int out_dim, int in_dim,
                            float in_scale, float w_scale, float out_scale) {
    // Use FIONA INT16 MVM
    photonic_mvm_int16(output, weight, input, out_dim, in_dim);

    // Rescale: MVM result is in (in_scale * w_scale) space
    float combined_scale = in_scale * w_scale;
    float rescale_factor = out_scale / combined_scale;

    for (int i = 0; i < out_dim; i++) {
        // Convert MVM result to target scale
        float val = (float)output[i] * rescale_factor;
        if (bias) {
            val += bias[i] * out_scale;  // Bias also needs to be scaled
        }
        // Clamp to INT16 range
        int32_t q = (int32_t)roundf(val);
        if (q > 32767) q = 32767;
        if (q < -32768) q = -32768;
        output[i] = (int16_t)q;
    }
}

void photonic_linear_2d_scaled(int16_t* output, const int16_t* input,
                               const int16_t* weight, const float* bias,
                               int seq_len, int out_dim, int in_dim,
                               float in_scale, float w_scale, float out_scale) {
    for (int t = 0; t < seq_len; t++) {
        photonic_linear_scaled(&output[t * out_dim], &input[t * in_dim],
                               weight, bias, out_dim, in_dim,
                               in_scale, w_scale, out_scale);
    }
}

// ============================================================
// Electronic Operations (LayerNorm, Softmax, GELU)
// ============================================================

void layer_norm_scaled(int16_t* output, const int16_t* input,
                       const float* gamma, const float* beta,
                       int dim, float in_scale, float out_scale) {
    float temp[D_MODEL];
    float mean = 0.0f;

    for (int i = 0; i < dim; i++) {
        temp[i] = dequant_s(input[i], in_scale);
        mean += temp[i];
    }
    mean /= dim;

    float var = 0.0f;
    for (int i = 0; i < dim; i++) {
        float diff = temp[i] - mean;
        var += diff * diff;
    }
    var /= dim;

    float inv_std = 1.0f / sqrtf(var + 1e-5f);
    for (int i = 0; i < dim; i++) {
        float normed = (temp[i] - mean) * inv_std * gamma[i] + beta[i];
        output[i] = quant_s(normed, out_scale);
    }
}

void layer_norm_2d_scaled(int16_t* output, const int16_t* input,
                          const float* gamma, const float* beta,
                          int seq_len, int dim,
                          float in_scale, float out_scale) {
    for (int t = 0; t < seq_len; t++) {
        layer_norm_scaled(&output[t * dim], &input[t * dim],
                          gamma, beta, dim, in_scale, out_scale);
    }
}

void softmax_row_scaled(int16_t* scores, int len, float in_scale, float out_scale) {
    float temp[MAX_SEQ_LEN];
    float max_val = dequant_s(scores[0], in_scale);
    for (int i = 0; i < len; i++) {
        temp[i] = dequant_s(scores[i], in_scale);
        if (temp[i] > max_val) max_val = temp[i];
    }

    float sum = 0.0f;
    for (int i = 0; i < len; i++) {
        temp[i] = expf(temp[i] - max_val);
        sum += temp[i];
    }

    for (int i = 0; i < len; i++) {
        // Softmax output is in [0, 1], scale to out_scale
        scores[i] = quant_s(temp[i] / sum, out_scale);
    }
}

float gelu_f(float x) {
    const float sqrt_2_over_pi = 0.7978845608f;
    float x3 = x * x * x;
    float inner = sqrt_2_over_pi * (x + 0.044715f * x3);
    return 0.5f * x * (1.0f + tanhf(inner));
}

// ============================================================
// Multi-Head Attention with Layer-Specific Scales
// ============================================================

void multihead_attention_photonic(
    int16_t* output, const int16_t* input, int seq_len,
    const int16_t* Wq, const float* bq,
    const int16_t* Wk, const float* bk,
    const int16_t* Wv, const float* bv,
    const int16_t* Wo, const float* bo,
    float input_scale,  // Scale of input tensor
    const int* attn_mask  // Attention mask (1 = valid, 0 = padding)
) {
    // QKV projections: input (input_scale) * weight (qkv_scale) -> output (qkv_scale)
    // Note: weights are quantized with qkv_scale for QKV projections
    photonic_linear_2d_scaled(Q_q, input, Wq, bq, seq_len, D_MODEL, D_MODEL,
                              input_scale, SCALES.qkv_scale, SCALES.qkv_scale);
    photonic_linear_2d_scaled(K_q, input, Wk, bk, seq_len, D_MODEL, D_MODEL,
                              input_scale, SCALES.qkv_scale, SCALES.qkv_scale);
    photonic_linear_2d_scaled(V_q, input, Wv, bv, seq_len, D_MODEL, D_MODEL,
                              input_scale, SCALES.qkv_scale, SCALES.qkv_scale);

    float scale_qk = 1.0f / sqrtf((float)D_K);

    // Attention computation (ELECTRONIC - full precision internally)
    for (int h = 0; h < N_HEADS; h++) {
        for (int i = 0; i < seq_len; i++) {
            for (int j = 0; j < seq_len; j++) {
                float score = 0.0f;
                for (int k = 0; k < D_K; k++) {
                    int q_idx = i * D_MODEL + h * D_K + k;
                    int k_idx = j * D_MODEL + h * D_K + k;
                    // Dequantize with qkv_scale
                    score += dequant_s(Q_q[q_idx], SCALES.qkv_scale) *
                             dequant_s(K_q[k_idx], SCALES.qkv_scale);
                }
                score *= scale_qk;

                // Apply attention mask: set padding positions to -inf
                if (attn_mask != NULL && attn_mask[j] == 0) {
                    score = -10000.0f;
                }

                // Store with a temporary scale before softmax
                attn_scores_q[h * seq_len * seq_len + i * seq_len + j] =
                    quant_s(score, SCALES.output_scale);
            }
            // Softmax: input (output_scale) -> output (softmax_scale for [0,1] precision)
            softmax_row_scaled(&attn_scores_q[h * seq_len * seq_len + i * seq_len],
                               seq_len, SCALES.output_scale, SCALES.softmax_scale);
        }

        // Attention weighted sum: scores (softmax_scale [0,1]) * V (qkv_scale)
        for (int i = 0; i < seq_len; i++) {
            for (int k = 0; k < D_K; k++) {
                float sum = 0.0f;
                for (int j = 0; j < seq_len; j++) {
                    // Score is [0, 1] stored with softmax_scale
                    float score = dequant_s(
                        attn_scores_q[h * seq_len * seq_len + i * seq_len + j],
                        SCALES.softmax_scale);
                    int v_idx = j * D_MODEL + h * D_K + k;
                    sum += score * dequant_s(V_q[v_idx], SCALES.qkv_scale);
                }
                // Output with attn_scale
                attn_out_q[i * D_MODEL + h * D_K + k] = quant_s(sum, SCALES.attn_scale);
            }
        }
    }

    // Output projection: attn_out (attn_scale) * Wo (output_scale) -> output (output_scale)
    photonic_linear_2d_scaled(output, attn_out_q, Wo, bo, seq_len, D_MODEL, D_MODEL,
                              SCALES.attn_scale, SCALES.output_scale, SCALES.output_scale);
}

// ============================================================
// Feed-Forward Network with Layer-Specific Scales
// ============================================================

void feed_forward_photonic(
    int16_t* output, const int16_t* input, int seq_len,
    const int16_t* W1, const float* b1,
    const int16_t* W2, const float* b2,
    float input_scale  // Scale of input tensor
) {
    // First linear: input (input_scale) * W1 (ffn_scale) -> hidden (ffn_scale)
    for (int t = 0; t < seq_len; t++) {
        photonic_linear_scaled(&ffn_hidden_q[t * D_FF], &input[t * D_MODEL],
                               W1, b1, D_FF, D_MODEL,
                               input_scale, SCALES.ffn_scale, SCALES.ffn_scale);

        // GELU activation (ELECTRONIC - full precision)
        for (int i = 0; i < D_FF; i++) {
            float val = dequant_s(ffn_hidden_q[t * D_FF + i], SCALES.ffn_scale);
            ffn_hidden_q[t * D_FF + i] = quant_s(gelu_f(val), SCALES.ffn_scale);
        }
    }

    // Second linear: hidden (ffn_scale) * W2 (output_scale) -> output (output_scale)
    photonic_linear_2d_scaled(output, ffn_hidden_q, W2, b2, seq_len, D_MODEL, D_FF,
                              SCALES.ffn_scale, SCALES.output_scale, SCALES.output_scale);
}

// ============================================================
// Transformer Encoder Layer with Scale-Aware Residual
// ============================================================

void transformer_encoder_layer_photonic(
    int16_t* output, const int16_t* input, int seq_len,
    const int16_t* Wq, const float* bq,
    const int16_t* Wk, const float* bk,
    const int16_t* Wv, const float* bv,
    const int16_t* Wo, const float* bo,
    const int16_t* W1, const float* b1,
    const int16_t* W2, const float* b2,
    const float* ln1_gamma, const float* ln1_beta,
    const float* ln2_gamma, const float* ln2_beta,
    float input_scale,  // Scale of input tensor
    const int* attn_mask  // Attention mask
) {
    // Post-LN (BERT style, norm_first=False)
    // 1. Self-attention + residual + LayerNorm
    multihead_attention_photonic(attn_proj_q, input, seq_len,
                                 Wq, bq, Wk, bk, Wv, bv, Wo, bo,
                                 input_scale, attn_mask);

    // Residual connection: input + attn_proj
    for (int i = 0; i < seq_len * D_MODEL; i++) {
        float in_val = dequant_s(input[i], input_scale);
        float attn_val = dequant_s(attn_proj_q[i], SCALES.output_scale);
        temp_q[i] = quant_s(in_val + attn_val, SCALES.output_scale);
    }

    // LayerNorm1 (after attention + residual)
    layer_norm_2d_scaled(normed_q, temp_q, ln1_gamma, ln1_beta,
                         seq_len, D_MODEL, SCALES.output_scale, SCALES.output_scale);

    // 2. FFN + residual + LayerNorm
    feed_forward_photonic(ffn_out_q, normed_q, seq_len, W1, b1, W2, b2,
                          SCALES.output_scale);

    // Residual connection: normed + ffn_out
    for (int i = 0; i < seq_len * D_MODEL; i++) {
        float normed_val = dequant_s(normed_q[i], SCALES.output_scale);
        float ffn_val = dequant_s(ffn_out_q[i], SCALES.output_scale);
        temp_q[i] = quant_s(normed_val + ffn_val, SCALES.output_scale);
    }

    // LayerNorm2 (after FFN + residual)
    layer_norm_2d_scaled(output, temp_q, ln2_gamma, ln2_beta,
                         seq_len, D_MODEL, SCALES.output_scale, SCALES.output_scale);
}

// ============================================================
// Embedding and Classification
// ============================================================

void embed_tokens_q(const int* token_ids, int seq_len, int16_t* output) {
    // BERT embedding: token + position + token_type (all zeros) + LayerNorm
    float temp_emb[MAX_SEQ_LEN * D_MODEL];

    for (int i = 0; i < seq_len; i++) {
        int token_id = token_ids[i];
        if (token_id < 0 || token_id >= MODEL_VOCAB_SIZE) token_id = 100;

        for (int j = 0; j < D_MODEL; j++) {
            temp_emb[i * D_MODEL + j] =
                token_embedding[token_id * D_MODEL + j] +
                position_embedding[i * D_MODEL + j] +
                token_type_embedding[j];  // token_type=0 for all tokens
        }
    }

    // Apply embedding LayerNorm (BERT-specific)
    for (int i = 0; i < seq_len; i++) {
        float mean = 0.0f;
        for (int j = 0; j < D_MODEL; j++) {
            mean += temp_emb[i * D_MODEL + j];
        }
        mean /= D_MODEL;

        float var = 0.0f;
        for (int j = 0; j < D_MODEL; j++) {
            float diff = temp_emb[i * D_MODEL + j] - mean;
            var += diff * diff;
        }
        var /= D_MODEL;

        float inv_std = 1.0f / sqrtf(var + 1e-12f);
        for (int j = 0; j < D_MODEL; j++) {
            float normed = (temp_emb[i * D_MODEL + j] - mean) * inv_std;
            float result = normed * embedding_ln_gamma[j] + embedding_ln_beta[j];
            output[i * D_MODEL + j] = quant_s(result, SCALES.embed_scale);
        }
    }
}

// Debug flag
static bool debug_classify = true;
static int debug_sample_count = 0;

int classify_photonic(const int16_t* encoder_output, float* probs) {
    bool do_debug = debug_classify && (debug_sample_count < 3);

    int16_t cls_hidden[D_MODEL];
    for (int i = 0; i < D_MODEL; i++) {
        cls_hidden[i] = encoder_output[i];
    }

    if (do_debug) {
        printf("\n    [DEBUG] CLS token (first 8, scale=%.0f): ", SCALES.output_scale);
        for (int i = 0; i < 8; i++) printf("%d ", cls_hidden[i]);
        printf("...\n");
        printf("    [DEBUG] CLS dequantized (first 8): ");
        for (int i = 0; i < 8; i++) printf("%.3f ", dequant_s(cls_hidden[i], SCALES.output_scale));
        printf("...\n");
    }

    // Pooler: cls_hidden (output_scale) * pooler_w (output_scale) -> pooled (output_scale)
    int16_t pooled_q[D_MODEL];
    photonic_linear_scaled(pooled_q, cls_hidden, pooler_w_q, pooler_bias,
                           D_MODEL, D_MODEL,
                           SCALES.output_scale, SCALES.output_scale, SCALES.output_scale);

    if (do_debug) {
        printf("    [DEBUG] After pooler linear (first 8): ");
        for (int i = 0; i < 8; i++) printf("%d ", pooled_q[i]);
        printf("...\n");
        int overflow = 0;
        for (int i = 0; i < D_MODEL; i++) {
            if (pooled_q[i] == 32767 || pooled_q[i] == -32768) overflow++;
        }
        if (overflow > 0) printf("    [DEBUG] Pooler overflow: %d/%d\n", overflow, D_MODEL);
    }

    // Tanh (ELECTRONIC - full precision)
    float pooled[D_MODEL];
    for (int i = 0; i < D_MODEL; i++) {
        pooled[i] = tanhf(dequant_s(pooled_q[i], SCALES.output_scale));
    }

    if (do_debug) {
        printf("    [DEBUG] After tanh (first 8): ");
        for (int i = 0; i < 8; i++) printf("%.3f ", pooled[i]);
        printf("...\n");
    }

    // Re-quantize for classifier
    int16_t pooled_req[D_MODEL];
    for (int i = 0; i < D_MODEL; i++) {
        pooled_req[i] = quant_s(pooled[i], SCALES.output_scale);
    }

    // Classifier: pooled (output_scale) * classifier_w (output_scale) -> logits (output_scale)
    int16_t logits_q[NUM_LABELS];
    photonic_linear_scaled(logits_q, pooled_req, classifier_w_q, classifier_bias,
                           NUM_LABELS, D_MODEL,
                           SCALES.output_scale, SCALES.output_scale, SCALES.output_scale);

    // Softmax (ELECTRONIC - full precision)
    float logits[NUM_LABELS];
    for (int i = 0; i < NUM_LABELS; i++) {
        logits[i] = dequant_s(logits_q[i], SCALES.output_scale);
    }

    if (do_debug) {
        printf("    [DEBUG] Logits (INT16): [%d, %d]\n", logits_q[0], logits_q[1]);
        printf("    [DEBUG] Logits (float): [%.4f, %.4f]\n", logits[0], logits[1]);
        debug_sample_count++;
    }

    float max_logit = logits[0];
    for (int i = 1; i < NUM_LABELS; i++) {
        if (logits[i] > max_logit) max_logit = logits[i];
    }

    float sum = 0.0f;
    for (int i = 0; i < NUM_LABELS; i++) {
        probs[i] = expf(logits[i] - max_logit);
        sum += probs[i];
    }
    for (int i = 0; i < NUM_LABELS; i++) {
        probs[i] /= sum;
    }

    return (probs[1] > probs[0]) ? 1 : 0;
}

// ============================================================
// Debug Utilities
// ============================================================

void print_stats(const char* name, const int16_t* arr, size_t len) {
    int16_t min_val = arr[0], max_val = arr[0];
    int64_t sum = 0;
    int overflow_count = 0;
    for (size_t i = 0; i < len; i++) {
        if (arr[i] < min_val) min_val = arr[i];
        if (arr[i] > max_val) max_val = arr[i];
        sum += arr[i];
        if (arr[i] == 32767 || arr[i] == -32768) overflow_count++;
    }
    float mean = (float)sum / len;
    printf("    %s: min=%d, max=%d, mean=%.2f, overflow=%d/%zu\n",
           name, min_val, max_val, mean, overflow_count, len);
}

void print_stats_f(const char* name, const float* arr, size_t len) {
    float min_val = arr[0], max_val = arr[0];
    double sum = 0;
    for (size_t i = 0; i < len; i++) {
        if (arr[i] < min_val) min_val = arr[i];
        if (arr[i] > max_val) max_val = arr[i];
        sum += arr[i];
    }
    float mean = (float)(sum / len);
    printf("    %s: min=%.4f, max=%.4f, mean=%.4f\n",
           name, min_val, max_val, mean);
}

// ============================================================
// Weight Quantization with Layer-Specific Scales
// ============================================================

void quantize_all_weights() {
    printf("  Quantizing weights with layer-specific scales...\n");
    printf("    embed_scale=%.1f, qkv_scale=%.1f, attn_scale=%.1f\n",
           SCALES.embed_scale, SCALES.qkv_scale, SCALES.attn_scale);
    printf("    ffn_scale=%.1f, output_scale=%.1f, softmax_scale=%.1f\n",
           SCALES.ffn_scale, SCALES.output_scale, SCALES.softmax_scale);

    // Check original weight ranges
    printf("\n  Original FP32 weight statistics:\n");
    print_stats_f("layer0_Wq", layer0_Wq, D_MODEL * D_MODEL);
    print_stats_f("layer0_W1", layer0_W1, D_MODEL * D_FF);
    print_stats_f("pooler_weight", pooler_weight, D_MODEL * D_MODEL);
    print_stats_f("classifier_weight", classifier_weight, NUM_LABELS * D_MODEL);

    // Layer 0 - QKV weights with qkv_scale
    quantize_weights_s(layer0_Wq_q, layer0_Wq, D_MODEL * D_MODEL, SCALES.qkv_scale);
    quantize_weights_s(layer0_Wk_q, layer0_Wk, D_MODEL * D_MODEL, SCALES.qkv_scale);
    quantize_weights_s(layer0_Wv_q, layer0_Wv, D_MODEL * D_MODEL, SCALES.qkv_scale);
    // Wo with output_scale (projects from attention back to residual)
    quantize_weights_s(layer0_Wo_q, layer0_Wo, D_MODEL * D_MODEL, SCALES.output_scale);
    // FFN weights
    quantize_weights_s(layer0_W1_q, layer0_W1, D_MODEL * D_FF, SCALES.ffn_scale);
    quantize_weights_s(layer0_W2_q, layer0_W2, D_FF * D_MODEL, SCALES.output_scale);

    // Layer 1 - same pattern
    quantize_weights_s(layer1_Wq_q, layer1_Wq, D_MODEL * D_MODEL, SCALES.qkv_scale);
    quantize_weights_s(layer1_Wk_q, layer1_Wk, D_MODEL * D_MODEL, SCALES.qkv_scale);
    quantize_weights_s(layer1_Wv_q, layer1_Wv, D_MODEL * D_MODEL, SCALES.qkv_scale);
    quantize_weights_s(layer1_Wo_q, layer1_Wo, D_MODEL * D_MODEL, SCALES.output_scale);
    quantize_weights_s(layer1_W1_q, layer1_W1, D_MODEL * D_FF, SCALES.ffn_scale);
    quantize_weights_s(layer1_W2_q, layer1_W2, D_FF * D_MODEL, SCALES.output_scale);

    // Classifier with output_scale
    quantize_weights_s(pooler_w_q, pooler_weight, D_MODEL * D_MODEL, SCALES.output_scale);
    quantize_weights_s(classifier_w_q, classifier_weight, NUM_LABELS * D_MODEL, SCALES.output_scale);

    printf("\n  Quantized INT16 weight statistics:\n");
    print_stats("layer0_Wq_q (qkv_scale)", layer0_Wq_q, D_MODEL * D_MODEL);
    print_stats("layer0_W1_q (ffn_scale)", layer0_W1_q, D_MODEL * D_FF);
    print_stats("pooler_w_q (output_scale)", pooler_w_q, D_MODEL * D_MODEL);
    print_stats("classifier_w_q (output_scale)", classifier_w_q, NUM_LABELS * D_MODEL);

    printf("  Done.\n\n");
}

// ============================================================
// Main Benchmark
// ============================================================

int main(int argc, char* argv[]) {
    // Parse command-line arguments for parallel execution
    int start_sample = 0;
    int end_sample = SST2_NUM_SAMPLES;

    if (argc >= 3) {
        start_sample = atoi(argv[1]);
        end_sample = atoi(argv[2]);
        if (start_sample < 0) start_sample = 0;
        if (end_sample > SST2_NUM_SAMPLES) end_sample = SST2_NUM_SAMPLES;
        if (start_sample >= end_sample) {
            printf("Error: start_sample >= end_sample\n");
            return 1;
        }
    }

    int num_samples = end_sample - start_sample;

    printf("============================================\n");
    printf("  TinySentiment SST-2 Benchmark (Photonic)\n");
    printf("============================================\n\n");

    printf("Model configuration:\n");
    printf("  d_model:      %d\n", D_MODEL);
    printf("  n_heads:      %d\n", N_HEADS);
    printf("  d_ff:         %d\n", D_FF);
    printf("  n_layers:     %d\n", N_LAYERS);
    printf("  vocab_size:   %d\n\n", MODEL_VOCAB_SIZE);

    printf("Quantization configuration:\n");
    printf("  embed_scale:   %.1f\n", SCALES.embed_scale);
    printf("  qkv_scale:     %.1f (smaller for dot product)\n", SCALES.qkv_scale);
    printf("  attn_scale:    %.1f\n", SCALES.attn_scale);
    printf("  ffn_scale:     %.1f\n", SCALES.ffn_scale);
    printf("  output_scale:  %.1f\n", SCALES.output_scale);
    printf("  softmax_scale: %.1f (for [0,1] precision)\n\n", SCALES.softmax_scale);

    printf("Evaluation set:\n");
    printf("  Dataset:      SST-2 validation\n");
    printf("  Samples:      %d (range: %d-%d)\n", num_samples, start_sample, end_sample);
    printf("  Seq length:   %d\n\n", SST2_SEQ_LEN);

    // Quantize all weights
    quantize_all_weights();

    printf("Running evaluation with FIONA photonic MVM...\n");
    printf("  [Debug enabled for first 3 samples]\n\n");

    int correct = 0;
    int true_pos = 0, true_neg = 0;
    int false_pos = 0, false_neg = 0;

    for (int t = start_sample; t < end_sample; t++) {
        // Get token IDs
        int token_ids[MAX_SEQ_LEN];
        for (int i = 0; i < SST2_SEQ_LEN && i < MAX_SEQ_LEN; i++) {
            token_ids[i] = sst2_token_ids[t][i];
        }

        // Generate attention mask (1 = valid, 0 = padding)
        for (int i = 0; i < SST2_SEQ_LEN; i++) {
            attention_mask[i] = (token_ids[i] != 0) ? 1 : 0;
        }

        // Embedding: output scale = embed_scale
        embed_tokens_q(token_ids, SST2_SEQ_LEN, embeddings_q);

        // Debug: check embedding stats for first sample
        if (t == 0) {
            printf("    [DEBUG] Embeddings (first sample):\n");
            print_stats("embeddings_q", embeddings_q, SST2_SEQ_LEN * D_MODEL);
        }

        // Layer 0: input scale = embed_scale, output scale = output_scale
        transformer_encoder_layer_photonic(
            hidden_q, embeddings_q, SST2_SEQ_LEN,
            layer0_Wq_q, layer0_bq, layer0_Wk_q, layer0_bk,
            layer0_Wv_q, layer0_bv, layer0_Wo_q, layer0_bo,
            layer0_W1_q, layer0_b1, layer0_W2_q, layer0_b2,
            layer0_ln1_gamma, layer0_ln1_beta,
            layer0_ln2_gamma, layer0_ln2_beta,
            SCALES.embed_scale,  // Input from embedding
            attention_mask
        );

        if (t == 0) {
            printf("    [DEBUG] After Layer 0:\n");
            print_stats("hidden_q", hidden_q, SST2_SEQ_LEN * D_MODEL);
        }

        // Layer 1: input scale = output_scale, output scale = output_scale
        transformer_encoder_layer_photonic(
            hidden2_q, hidden_q, SST2_SEQ_LEN,
            layer1_Wq_q, layer1_bq, layer1_Wk_q, layer1_bk,
            layer1_Wv_q, layer1_bv, layer1_Wo_q, layer1_bo,
            layer1_W1_q, layer1_b1, layer1_W2_q, layer1_b2,
            layer1_ln1_gamma, layer1_ln1_beta,
            layer1_ln2_gamma, layer1_ln2_beta,
            SCALES.output_scale,  // Input from layer 0
            attention_mask
        );

        if (t == 0) {
            printf("    [DEBUG] After Layer 1:\n");
            print_stats("hidden2_q", hidden2_q, SST2_SEQ_LEN * D_MODEL);
        }

        // Classification
        float probs[NUM_LABELS];
        int pred = classify_photonic(hidden2_q, probs);
        int label = sst2_labels[t];

        if (pred == label) {
            correct++;
            if (pred == 1) true_pos++;
            else true_neg++;
        } else {
            if (pred == 1) false_pos++;
            else false_neg++;
        }

        // 各サンプル処理後にログ出力
        int processed = t - start_sample + 1;
        float acc_now = 100.0f * correct / processed;
        printf("  [%4d/%4d] (idx=%d) pred=%d label=%d %s | Acc: %.2f%% (%d/%d)\n",
               processed, num_samples, t,
               pred, label,
               (pred == label) ? "OK" : "NG",
               acc_now, correct, processed);
    }

    // Calculate metrics
    float accuracy = 100.0f * correct / num_samples;
    float precision = (true_pos + false_pos > 0) ?
        100.0f * true_pos / (true_pos + false_pos) : 0.0f;
    float recall = (true_pos + false_neg > 0) ?
        100.0f * true_pos / (true_pos + false_neg) : 0.0f;
    float f1 = (precision + recall > 0) ?
        2.0f * precision * recall / (precision + recall) : 0.0f;

    printf("\n============================================\n");
    printf("  RESULTS (Photonic INT16)\n");
    printf("  Sample range: %d-%d (%d samples)\n", start_sample, end_sample, num_samples);
    printf("============================================\n\n");

    printf("Confusion Matrix:\n");
    printf("                 Predicted\n");
    printf("              NEG      POS\n");
    printf("  Actual NEG  %4d     %4d\n", true_neg, false_pos);
    printf("         POS  %4d     %4d\n\n", false_neg, true_pos);

    printf("Metrics:\n");
    printf("  Accuracy:   %.2f%% (%d/%d)\n", accuracy, correct, num_samples);
    printf("  Precision:  %.2f%%\n", precision);
    printf("  Recall:     %.2f%%\n", recall);
    printf("  F1 Score:   %.2f%%\n", f1);

    printf("\n============================================\n");

    return 0;
}
