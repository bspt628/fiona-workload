#!/usr/bin/env python3
"""
prepare_sst2_testset.py - Prepare SST-2 validation set for FIONA evaluation

Exports pre-tokenized SST-2 sentences as C header for comprehensive evaluation.

Usage:
    python prepare_sst2_testset.py --output_dir ../app/sentiment_classifier/testdata

Author: FIONA Project
Date: 2025-12-23
"""

import argparse
from pathlib import Path
from datasets import load_dataset
from transformers import BertTokenizer


def main():
    parser = argparse.ArgumentParser(description="Prepare SST-2 test set for FIONA")
    parser.add_argument("--output_dir", type=str, default="../app/sentiment_classifier/testdata",
                        help="Output directory for test data")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="Maximum number of samples (None = all)")
    parser.add_argument("--max_seq_len", type=int, default=64,
                        help="Maximum sequence length")
    args = parser.parse_args()

    print("=" * 60)
    print("SST-2 Test Set Preparation for FIONA")
    print("=" * 60)

    # Load SST-2 validation set
    print("\nLoading SST-2 dataset...")
    dataset = load_dataset("glue", "sst2", split="validation")

    if args.max_samples:
        dataset = dataset.select(range(min(args.max_samples, len(dataset))))

    print(f"  Total samples: {len(dataset)}")

    # Load tokenizer
    print("Loading BERT tokenizer...")
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # Create output directory
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Tokenize all sentences
    print("Tokenizing sentences...")
    samples = []
    for i, example in enumerate(dataset):
        text = example["sentence"]
        label = example["label"]

        # Tokenize
        encoding = tokenizer(
            text,
            max_length=args.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="np"
        )

        token_ids = encoding["input_ids"][0].tolist()
        samples.append({
            "text": text,
            "label": label,
            "token_ids": token_ids
        })

    # Write C header with test data
    print(f"Writing test data to {output_path}...")

    with open(output_path / "sst2_testdata.h", "w") as f:
        f.write("/**\n")
        f.write(" * @file sst2_testdata.h\n")
        f.write(" * @brief SST-2 validation set for FIONA evaluation\n")
        f.write(" * @note Auto-generated by prepare_sst2_testset.py\n")
        f.write(f" * @note Total samples: {len(samples)}\n")
        f.write(" */\n\n")
        f.write("#ifndef SST2_TESTDATA_H\n")
        f.write("#define SST2_TESTDATA_H\n\n")

        f.write(f"#define SST2_NUM_SAMPLES {len(samples)}\n")
        f.write(f"#define SST2_SEQ_LEN {args.max_seq_len}\n\n")

        # Write token IDs
        f.write("// Token IDs [num_samples][seq_len]\n")
        f.write(f"static const int sst2_token_ids[{len(samples)}][{args.max_seq_len}] = {{\n")
        for i, sample in enumerate(samples):
            ids_str = ", ".join(str(x) for x in sample["token_ids"])
            f.write(f"    {{{ids_str}}},\n")
        f.write("};\n\n")

        # Write labels
        f.write("// Labels (0=negative, 1=positive)\n")
        f.write(f"static const int sst2_labels[{len(samples)}] = {{\n    ")
        labels = [str(s["label"]) for s in samples]
        for i in range(0, len(labels), 20):
            f.write(", ".join(labels[i:i+20]))
            if i + 20 < len(labels):
                f.write(",\n    ")
        f.write("\n};\n\n")

        f.write("#endif // SST2_TESTDATA_H\n")

    # Write sentence texts for reference
    with open(output_path / "sst2_sentences.txt", "w") as f:
        for i, sample in enumerate(samples):
            f.write(f"{i}\t{sample['label']}\t{sample['text']}\n")

    # Statistics
    pos_count = sum(1 for s in samples if s["label"] == 1)
    neg_count = len(samples) - pos_count

    print(f"\n  Positive samples: {pos_count} ({100*pos_count/len(samples):.1f}%)")
    print(f"  Negative samples: {neg_count} ({100*neg_count/len(samples):.1f}%)")
    print(f"\nOutput files:")
    print(f"  - {output_path / 'sst2_testdata.h'}")
    print(f"  - {output_path / 'sst2_sentences.txt'}")
    print("=" * 60)


if __name__ == "__main__":
    main()
