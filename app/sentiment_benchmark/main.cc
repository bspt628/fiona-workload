/**
 * @file main_benchmark.cc
 * @brief Sentiment Classification Benchmark - Full SST-2 Evaluation
 *
 * This version evaluates the model on the complete SST-2 validation set
 * for paper-quality results.
 *
 * @author FIONA Project
 * @date 2025-12-23
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

#include "fiona.h"
#include "nn/transformer.h"

// Include exported weights
#include "weights/weights.h"

// Include test data (generated by prepare_sst2_testset.py)
#include "testdata/sst2_testdata.h"

// ============================================================
// Model Configuration
// ============================================================

#define D_MODEL        128
#define N_HEADS        2
#define D_K            (D_MODEL / N_HEADS)
#define D_FF           256
#define N_LAYERS       2
#define MAX_SEQ_LEN    64
#define NUM_LABELS     2
#define VOCAB_SIZE     30522

// ============================================================
// Static Buffers
// ============================================================

static float embeddings[MAX_SEQ_LEN * D_MODEL];
static float hidden[MAX_SEQ_LEN * D_MODEL];
static float hidden2[MAX_SEQ_LEN * D_MODEL];
static float temp[MAX_SEQ_LEN * D_MODEL];
static float normed[MAX_SEQ_LEN * D_MODEL];

static float Q[MAX_SEQ_LEN * D_MODEL];
static float K[MAX_SEQ_LEN * D_MODEL];
static float V[MAX_SEQ_LEN * D_MODEL];
static float attn_scores[N_HEADS * MAX_SEQ_LEN * MAX_SEQ_LEN];
static float attn_out[MAX_SEQ_LEN * D_MODEL];
static float attn_proj[MAX_SEQ_LEN * D_MODEL];

static float ffn_hidden[MAX_SEQ_LEN * D_FF];
static float ffn_out[MAX_SEQ_LEN * D_MODEL];

// ============================================================
// PyTorch-compatible implementations (same as main.cc)
// ============================================================

void pytorch_linear(float* output, const float* input,
                    const float* weight, const float* bias,
                    int out_dim, int in_dim) {
    for (int i = 0; i < out_dim; i++) {
        float sum = 0.0f;
        for (int j = 0; j < in_dim; j++) {
            sum += input[j] * weight[i * in_dim + j];
        }
        output[i] = sum + (bias ? bias[i] : 0.0f);
    }
}

void pytorch_linear_2d(float* output, const float* input,
                       const float* weight, const float* bias,
                       int seq_len, int out_dim, int in_dim) {
    for (int t = 0; t < seq_len; t++) {
        pytorch_linear(&output[t * out_dim], &input[t * in_dim],
                       weight, bias, out_dim, in_dim);
    }
}

void layer_norm(float* output, const float* input,
                const float* gamma, const float* beta,
                int dim, float eps) {
    float mean = 0.0f;
    for (int i = 0; i < dim; i++) mean += input[i];
    mean /= dim;

    float var = 0.0f;
    for (int i = 0; i < dim; i++) {
        float diff = input[i] - mean;
        var += diff * diff;
    }
    var /= dim;

    float inv_std = 1.0f / sqrtf(var + eps);
    for (int i = 0; i < dim; i++) {
        output[i] = (input[i] - mean) * inv_std * gamma[i] + beta[i];
    }
}

void layer_norm_2d(float* output, const float* input,
                   const float* gamma, const float* beta,
                   int seq_len, int dim, float eps) {
    for (int t = 0; t < seq_len; t++) {
        layer_norm(&output[t * dim], &input[t * dim], gamma, beta, dim, eps);
    }
}

void softmax_row(float* scores, int len) {
    float max_val = scores[0];
    for (int i = 1; i < len; i++) {
        if (scores[i] > max_val) max_val = scores[i];
    }

    float sum = 0.0f;
    for (int i = 0; i < len; i++) {
        scores[i] = expf(scores[i] - max_val);
        sum += scores[i];
    }

    for (int i = 0; i < len; i++) {
        scores[i] /= sum;
    }
}

void multihead_attention(
    float* output, const float* input, int seq_len,
    const float* Wq, const float* bq,
    const float* Wk, const float* bk,
    const float* Wv, const float* bv,
    const float* Wo, const float* bo
) {
    pytorch_linear_2d(Q, input, Wq, bq, seq_len, D_MODEL, D_MODEL);
    pytorch_linear_2d(K, input, Wk, bk, seq_len, D_MODEL, D_MODEL);
    pytorch_linear_2d(V, input, Wv, bv, seq_len, D_MODEL, D_MODEL);

    float scale = 1.0f / sqrtf((float)D_K);

    for (int h = 0; h < N_HEADS; h++) {
        for (int i = 0; i < seq_len; i++) {
            for (int j = 0; j < seq_len; j++) {
                float score = 0.0f;
                for (int k = 0; k < D_K; k++) {
                    int q_idx = i * D_MODEL + h * D_K + k;
                    int k_idx = j * D_MODEL + h * D_K + k;
                    score += Q[q_idx] * K[k_idx];
                }
                attn_scores[h * seq_len * seq_len + i * seq_len + j] = score * scale;
            }
            softmax_row(&attn_scores[h * seq_len * seq_len + i * seq_len], seq_len);
        }

        for (int i = 0; i < seq_len; i++) {
            for (int k = 0; k < D_K; k++) {
                float sum = 0.0f;
                for (int j = 0; j < seq_len; j++) {
                    float score = attn_scores[h * seq_len * seq_len + i * seq_len + j];
                    int v_idx = j * D_MODEL + h * D_K + k;
                    sum += score * V[v_idx];
                }
                attn_out[i * D_MODEL + h * D_K + k] = sum;
            }
        }
    }

    pytorch_linear_2d(output, attn_out, Wo, bo, seq_len, D_MODEL, D_MODEL);
}

float gelu(float x) {
    const float sqrt_2_over_pi = 0.7978845608f;
    float x3 = x * x * x;
    float inner = sqrt_2_over_pi * (x + 0.044715f * x3);
    return 0.5f * x * (1.0f + tanhf(inner));
}

void feed_forward(
    float* output, const float* input, int seq_len,
    const float* W1, const float* b1,
    const float* W2, const float* b2
) {
    for (int t = 0; t < seq_len; t++) {
        pytorch_linear(&ffn_hidden[t * D_FF], &input[t * D_MODEL],
                       W1, b1, D_FF, D_MODEL);
        for (int i = 0; i < D_FF; i++) {
            ffn_hidden[t * D_FF + i] = gelu(ffn_hidden[t * D_FF + i]);
        }
    }
    pytorch_linear_2d(output, ffn_hidden, W2, b2, seq_len, D_MODEL, D_FF);
}

void transformer_encoder_layer(
    float* output, const float* input, int seq_len,
    const float* Wq, const float* bq,
    const float* Wk, const float* bk,
    const float* Wv, const float* bv,
    const float* Wo, const float* bo,
    const float* W1, const float* b1,
    const float* W2, const float* b2,
    const float* ln1_gamma, const float* ln1_beta,
    const float* ln2_gamma, const float* ln2_beta
) {
    layer_norm_2d(normed, input, ln1_gamma, ln1_beta, seq_len, D_MODEL, 1e-5f);
    multihead_attention(attn_proj, normed, seq_len, Wq, bq, Wk, bk, Wv, bv, Wo, bo);

    for (int i = 0; i < seq_len * D_MODEL; i++) {
        temp[i] = input[i] + attn_proj[i];
    }

    layer_norm_2d(normed, temp, ln2_gamma, ln2_beta, seq_len, D_MODEL, 1e-5f);
    feed_forward(ffn_out, normed, seq_len, W1, b1, W2, b2);

    for (int i = 0; i < seq_len * D_MODEL; i++) {
        output[i] = temp[i] + ffn_out[i];
    }
}

void embed_tokens(const int* token_ids, int seq_len, float* output) {
    for (int i = 0; i < seq_len; i++) {
        int token_id = token_ids[i];
        if (token_id < 0 || token_id >= VOCAB_SIZE) token_id = 100;

        for (int j = 0; j < D_MODEL; j++) {
            output[i * D_MODEL + j] =
                token_embedding[token_id * D_MODEL + j] +
                position_embedding[i * D_MODEL + j];
        }
    }
}

int classify(const float* encoder_output, float* probs) {
    float cls_hidden[D_MODEL];
    for (int i = 0; i < D_MODEL; i++) {
        cls_hidden[i] = encoder_output[i];
    }

    float pooled[D_MODEL];
    pytorch_linear(pooled, cls_hidden, pooler_weight, pooler_bias, D_MODEL, D_MODEL);
    for (int i = 0; i < D_MODEL; i++) {
        pooled[i] = tanhf(pooled[i]);
    }

    float logits[NUM_LABELS];
    pytorch_linear(logits, pooled, classifier_weight, classifier_bias, NUM_LABELS, D_MODEL);

    float max_logit = logits[0];
    for (int i = 1; i < NUM_LABELS; i++) {
        if (logits[i] > max_logit) max_logit = logits[i];
    }

    float sum = 0.0f;
    for (int i = 0; i < NUM_LABELS; i++) {
        probs[i] = expf(logits[i] - max_logit);
        sum += probs[i];
    }
    for (int i = 0; i < NUM_LABELS; i++) {
        probs[i] /= sum;
    }

    return (probs[1] > probs[0]) ? 1 : 0;
}

// ============================================================
// Main Benchmark
// ============================================================

int main() {
    printf("============================================\n");
    printf("  TinySentiment SST-2 Benchmark\n");
    printf("============================================\n\n");

    printf("Model configuration:\n");
    printf("  d_model:      %d\n", D_MODEL);
    printf("  n_heads:      %d\n", N_HEADS);
    printf("  d_ff:         %d\n", D_FF);
    printf("  n_layers:     %d\n", N_LAYERS);
    printf("  vocab_size:   %d\n\n", VOCAB_SIZE);

    printf("Evaluation set:\n");
    printf("  Dataset:      SST-2 validation\n");
    printf("  Samples:      %d\n", SST2_NUM_SAMPLES);
    printf("  Seq length:   %d\n\n", SST2_SEQ_LEN);

    printf("Running evaluation...\n");

    int correct = 0;
    int true_pos = 0, true_neg = 0;
    int false_pos = 0, false_neg = 0;

    // Progress reporting interval
    int report_interval = SST2_NUM_SAMPLES / 10;
    if (report_interval == 0) report_interval = 1;

    for (int t = 0; t < SST2_NUM_SAMPLES; t++) {
        // Progress report
        if ((t + 1) % report_interval == 0 || t == SST2_NUM_SAMPLES - 1) {
            printf("  Progress: %d/%d (%.1f%%)\n",
                   t + 1, SST2_NUM_SAMPLES, 100.0f * (t + 1) / SST2_NUM_SAMPLES);
        }

        // Get token IDs for this sample
        int token_ids[MAX_SEQ_LEN];
        for (int i = 0; i < SST2_SEQ_LEN && i < MAX_SEQ_LEN; i++) {
            token_ids[i] = sst2_token_ids[t][i];
        }

        // Forward pass
        embed_tokens(token_ids, SST2_SEQ_LEN, embeddings);

        transformer_encoder_layer(
            hidden, embeddings, SST2_SEQ_LEN,
            layer0_Wq, layer0_bq, layer0_Wk, layer0_bk,
            layer0_Wv, layer0_bv, layer0_Wo, layer0_bo,
            layer0_W1, layer0_b1, layer0_W2, layer0_b2,
            layer0_ln1_gamma, layer0_ln1_beta,
            layer0_ln2_gamma, layer0_ln2_beta
        );

        transformer_encoder_layer(
            hidden2, hidden, SST2_SEQ_LEN,
            layer1_Wq, layer1_bq, layer1_Wk, layer1_bk,
            layer1_Wv, layer1_bv, layer1_Wo, layer1_bo,
            layer1_W1, layer1_b1, layer1_W2, layer1_b2,
            layer1_ln1_gamma, layer1_ln1_beta,
            layer1_ln2_gamma, layer1_ln2_beta
        );

        float probs[NUM_LABELS];
        int pred = classify(hidden2, probs);
        int label = sst2_labels[t];

        if (pred == label) {
            correct++;
            if (pred == 1) true_pos++;
            else true_neg++;
        } else {
            if (pred == 1) false_pos++;
            else false_neg++;
        }
    }

    // Calculate metrics
    float accuracy = 100.0f * correct / SST2_NUM_SAMPLES;
    float precision = (true_pos + false_pos > 0) ?
        100.0f * true_pos / (true_pos + false_pos) : 0.0f;
    float recall = (true_pos + false_neg > 0) ?
        100.0f * true_pos / (true_pos + false_neg) : 0.0f;
    float f1 = (precision + recall > 0) ?
        2.0f * precision * recall / (precision + recall) : 0.0f;

    printf("\n============================================\n");
    printf("  RESULTS\n");
    printf("============================================\n\n");

    printf("Confusion Matrix:\n");
    printf("                 Predicted\n");
    printf("              NEG      POS\n");
    printf("  Actual NEG  %4d     %4d\n", true_neg, false_pos);
    printf("         POS  %4d     %4d\n\n", false_neg, true_pos);

    printf("Metrics:\n");
    printf("  Accuracy:   %.2f%% (%d/%d)\n", accuracy, correct, SST2_NUM_SAMPLES);
    printf("  Precision:  %.2f%%\n", precision);
    printf("  Recall:     %.2f%%\n", recall);
    printf("  F1 Score:   %.2f%%\n", f1);

    printf("\n============================================\n");

    return 0;
}
