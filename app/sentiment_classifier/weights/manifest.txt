# Pre-trained Sentiment Model Weight Manifest
# Auto-generated by export_sentiment_pretrained.py
#
# Base model: prajjwal1/bert-tiny
# Validation accuracy: 0.8005
#
# Format: name, shape, dtype, size_bytes

token_embedding.weight, [30522, 128], float32, 15627264
position_embedding.weight, [512, 128], float32, 262144
token_type_embedding.weight, [2, 128], float32, 1024
embedding_layernorm.weight, [128], float32, 512
embedding_layernorm.bias, [128], float32, 512
encoder.layers.0.self_attn.in_proj_weight, [384, 128], float32, 196608
encoder.layers.0.self_attn.in_proj_bias, [384], float32, 1536
encoder.layers.0.self_attn.out_proj.weight, [128, 128], float32, 65536
encoder.layers.0.self_attn.out_proj.bias, [128], float32, 512
encoder.layers.0.linear1.weight, [512, 128], float32, 262144
encoder.layers.0.linear1.bias, [512], float32, 2048
encoder.layers.0.linear2.weight, [128, 512], float32, 262144
encoder.layers.0.linear2.bias, [128], float32, 512
encoder.layers.0.norm1.weight, [128], float32, 512
encoder.layers.0.norm1.bias, [128], float32, 512
encoder.layers.0.norm2.weight, [128], float32, 512
encoder.layers.0.norm2.bias, [128], float32, 512
encoder.layers.1.self_attn.in_proj_weight, [384, 128], float32, 196608
encoder.layers.1.self_attn.in_proj_bias, [384], float32, 1536
encoder.layers.1.self_attn.out_proj.weight, [128, 128], float32, 65536
encoder.layers.1.self_attn.out_proj.bias, [128], float32, 512
encoder.layers.1.linear1.weight, [512, 128], float32, 262144
encoder.layers.1.linear1.bias, [512], float32, 2048
encoder.layers.1.linear2.weight, [128, 512], float32, 262144
encoder.layers.1.linear2.bias, [128], float32, 512
encoder.layers.1.norm1.weight, [128], float32, 512
encoder.layers.1.norm1.bias, [128], float32, 512
encoder.layers.1.norm2.weight, [128], float32, 512
encoder.layers.1.norm2.bias, [128], float32, 512
pooler.weight, [128, 128], float32, 65536
pooler.bias, [128], float32, 512
classifier.weight, [2, 128], float32, 1024
classifier.bias, [2], float32, 8

# Configuration
# vocab_size: 30522
# max_position: 512
# d_model: 128
# n_heads: 2
# d_k: 64
# d_ff: 512
# n_layers: 2
# num_labels: 2
# dropout: 0.1

# Summary
# Total parameters: 4,386,178
# Total size (FP32): 16.73 MB
# Total size (INT16): 8.37 MB
